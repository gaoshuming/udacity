{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 图像分类\n",
    "\n",
    "在此项目中，你将对 [CIFAR-10 数据集](https://www.cs.toronto.edu/~kriz/cifar.html) 中的图片进行分类。该数据集包含飞机、猫狗和其他物体。你需要预处理这些图片，然后用所有样本训练一个卷积神经网络。图片需要标准化（normalized），标签需要采用 one-hot 编码。你需要应用所学的知识构建卷积的、最大池化（max pooling）、丢弃（dropout）和完全连接（fully connected）的层。最后，你需要在样本图片上看到神经网络的预测结果。\n",
    "\n",
    "\n",
    "## 获取数据\n",
    "\n",
    "请运行以下单元，以下载 [CIFAR-10 数据集（Python版）](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索数据\n",
    "\n",
    "该数据集分成了几部分／批次（batches），以免你的机器在计算时内存不足。CIFAR-10 数据集包含 5 个部分，名称分别为 `data_batch_1`、`data_batch_2`，以此类推。每个部分都包含以下某个类别的标签和图片：\n",
    "\n",
    "* 飞机\n",
    "* 汽车\n",
    "* 鸟类\n",
    "* 猫\n",
    "* 鹿\n",
    "* 狗\n",
    "* 青蛙\n",
    "* 马\n",
    "* 船只\n",
    "* 卡车\n",
    "\n",
    "了解数据集也是对数据进行预测的必经步骤。你可以通过更改 `batch_id` 和 `sample_id` 探索下面的代码单元。`batch_id` 是数据集一个部分的 ID（1 到 5）。`sample_id` 是该部分中图片和标签对（label pair）的 ID。\n",
    "\n",
    "问问你自己：“可能的标签有哪些？”、“图片数据的值范围是多少？”、“标签是按顺序排列，还是随机排列的？”。思考类似的问题，有助于你预处理数据，并使预测结果更准确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 50:\n",
      "Image - Min Value: 8 Max Value: 243\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 9 Name: truck\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4f8004a4a8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 50\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现预处理函数\n",
    "\n",
    "### 标准化\n",
    "\n",
    "在下面的单元中，实现 `normalize` 函数，传入图片数据 `x`，并返回标准化 Numpy 数组。值应该在 0 到 1 的范围内（含 0 和 1）。返回对象应该和 `x` 的形状一样。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return (x / 255)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot 编码\n",
    "\n",
    "和之前的代码单元一样，你将为预处理实现一个函数。这次，你将实现 `one_hot_encode` 函数。输入，也就是 `x`，是一个标签列表。实现该函数，以返回为 one_hot 编码的 Numpy 数组的标签列表。标签的可能值为 0 到 9。每次调用 `one_hot_encode` 时，对于每个值，one_hot 编码函数应该返回相同的编码。确保将编码映射保存到该函数外面。\n",
    "\n",
    "提示：不要重复发明轮子。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机化数据\n",
    "\n",
    "之前探索数据时，你已经了解到，样本的顺序是随机的。再随机化一次也不会有什么关系，但是对于这个数据集没有必要。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理所有数据并保存\n",
    "\n",
    "运行下方的代码单元，将预处理所有 CIFAR-10 数据，并保存到文件中。下面的代码还使用了 10% 的训练数据，用来验证。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "这是你的第一个检查点。如果你什么时候决定再回到该记事本，或需要重新启动该记事本，你可以从这里开始。预处理的数据已保存到本地。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络\n",
    "\n",
    "对于该神经网络，你需要将每层都构建为一个函数。你看到的大部分代码都位于函数外面。要更全面地测试你的代码，我们需要你将每层放入一个函数中。这样使我们能够提供更好的反馈，并使用我们的统一测试检测简单的错误，然后再提交项目。\n",
    "\n",
    ">**注意**：如果你觉得每周很难抽出足够的时间学习这门课程，我们为此项目提供了一个小捷径。对于接下来的几个问题，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 程序包中的类来构建每个层级，但是“卷积和最大池化层级”部分的层级除外。TF Layers 和 Keras 及 TFLearn 层级类似，因此很容易学会。\n",
    "\n",
    ">但是，如果你想充分利用这门课程，请尝试自己解决所有问题，不使用 TF Layers 程序包中的任何类。你依然可以使用其他程序包中的类，这些类和你在 TF Layers 中的类名称是一样的！例如，你可以使用 TF Neural Network 版本的 `conv2d` 类 [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)，而不是 TF Layers 版本的 `conv2d` 类 [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d)。\n",
    "\n",
    "我们开始吧！\n",
    "\n",
    "\n",
    "### 输入\n",
    "\n",
    "神经网络需要读取图片数据、one-hot 编码标签和丢弃保留概率（dropout keep probability）。请实现以下函数：\n",
    "\n",
    "* 实现 `neural_net_image_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * 使用 `image_shape` 设置形状，部分大小设为 `None`\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"x\" 命名\n",
    "* 实现 `neural_net_label_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * 使用 `n_classes` 设置形状，部分大小设为 `None`\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"y\" 命名\n",
    "* 实现 `neural_net_keep_prob_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)，用于丢弃保留概率\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"keep_prob\" 命名\n",
    "\n",
    "这些名称将在项目结束时，用于加载保存的模型。\n",
    "\n",
    "注意：TensorFlow 中的 `None` 表示形状可以是动态大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = (None, *image_shape), name = \"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.int8, shape = (None, n_classes), name = \"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = None, name = \"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积和最大池化层\n",
    "\n",
    "卷积层级适合处理图片。对于此代码单元，你应该实现函数 `conv2d_maxpool` 以便应用卷积然后进行最大池化：\n",
    "\n",
    "* 使用 `conv_ksize`、`conv_num_outputs` 和 `x_tensor` 的形状创建权重（weight）和偏置（bias）。\n",
    "* 使用权重和 `conv_strides` 对 `x_tensor` 应用卷积。\n",
    " * 建议使用我们建议的间距（padding），当然也可以使用任何其他间距。\n",
    "* 添加偏置\n",
    "* 向卷积中添加非线性激活（nonlinear activation）\n",
    "* 使用 `pool_ksize` 和 `pool_strides` 应用最大池化\n",
    " * 建议使用我们建议的间距（padding），当然也可以使用任何其他间距。\n",
    "\n",
    "**注意**：对于**此层**，**请勿使用** [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers)，但是仍然可以使用 TensorFlow 的 [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) 包。对于所有**其他层**，你依然可以使用快捷方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_chanel = int(x_tensor.shape[3])\n",
    "    output_chanel = conv_num_outputs\n",
    "    weight_shape = (*conv_ksize,input_chanel,output_chanel)  # * \n",
    "    weight = tf.Variable(tf.random_normal(weight_shape, stddev = 0.1))  #权重\n",
    "    bias = tf.Variable(tf.zeros(output_chanel))  #设置偏置项\n",
    "    l_active = tf.nn.conv2d(x_tensor, weight, (1, *conv_strides, 1), 'SAME')\n",
    "    l_active = tf.nn.bias_add(l_active,bias)\n",
    "    #active_layers = tf.nn.relu(tf.add(tf.matmul(features,label),bias))    #ReLu\n",
    "    mx_layer = tf.nn.relu(l_active)\n",
    "    \n",
    "    return tf.nn.max_pool(mx_layer, (1, *pool_ksize, 1), (1, *pool_strides, 1), 'VALID')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扁平化层\n",
    "\n",
    "实现 `flatten` 函数，将 `x_tensor` 的维度从四维张量（4-D tensor）变成二维张量。输出应该是形状（*部分大小（Batch Size）*，*扁平化图片大小（Flattened Image Size）*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    _, *image_size = x_tensor.get_shape().as_list()\n",
    "    #print(*image_size)\n",
    "    return tf.reshape(x_tensor, (-1, reduce(mul, image_size)))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完全连接的层\n",
    "\n",
    "实现 `fully_conn` 函数，以向 `x_tensor` 应用完全连接的层级，形状为（*部分大小（Batch Size）*，*num_outputs*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    num_input = x_tensor.get_shape().as_list()[1]\n",
    "    weight_shape = (num_input, num_outputs)\n",
    "    #print(weight_shape)\n",
    "    weight = tf.Variable(tf.truncated_normal(weight_shape, stddev = 0.1))\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "     \n",
    "    activation = tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "    return tf.nn.relu(activation)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出层\n",
    "\n",
    "实现 `output` 函数，向 x_tensor 应用完全连接的层级，形状为（*部分大小（Batch Size）*，*num_outputs*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。\n",
    "\n",
    "**注意**：该层级不应应用 Activation、softmax 或交叉熵（cross entropy）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    num_input = x_tensor.get_shape().as_list()[1] #not 0\n",
    "    weight_shape = (num_input, num_outputs) \n",
    "    weight = tf.Variable(tf.truncated_normal(weight_shape, stddev = 0.1))\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    return tf.nn.bias_add(tf.matmul(x_tensor,weight),bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建卷积模型\n",
    "\n",
    "实现函数 `conv_net`， 创建卷积神经网络模型。该函数传入一批图片 `x`，并输出对数（logits）。使用你在上方创建的层创建此模型：\n",
    "\n",
    "* 应用 1、2 或 3 个卷积和最大池化层（Convolution and Max Pool layers）\n",
    "* 应用一个扁平层（Flatten Layer）\n",
    "* 应用 1、2 或 3 个完全连接层（Fully Connected Layers）\n",
    "* 应用一个输出层（Output Layer）\n",
    "* 返回输出\n",
    "* 使用 `keep_prob` 向模型中的一个或多个层应用 [TensorFlow 的 Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x = conv2d_maxpool(x, 64, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    x = conv2d_maxpool(x, 128, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    # x has shape (batch, 8, 8, 128)\n",
    "    x = conv2d_maxpool(x, 256, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    x = flatten(x)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    x = fully_conn(x, 512)\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return output(x, 10)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络\n",
    "\n",
    "### 单次优化\n",
    "\n",
    "实现函数 `train_neural_network` 以进行单次优化（single optimization）。该优化应该使用 `optimizer` 优化 `session`，其中 `feed_dict` 具有以下参数：\n",
    "\n",
    "* `x` 表示图片输入\n",
    "* `y` 表示标签\n",
    "* `keep_prob` 表示丢弃的保留率\n",
    "\n",
    "每个部分都会调用该函数，所以 `tf.global_variables_initializer()` 已经被调用。\n",
    "\n",
    "注意：不需要返回任何内容。该函数只是用来优化神经网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示数据\n",
    "\n",
    "实现函数 `print_stats` 以输出损失和验证准确率。使用全局变量 `valid_features` 和 `valid_labels` 计算验证准确率。使用保留率 `1.0` 计算损失和验证准确率（loss and validation accuracy）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    global valid_features, valid_labels\n",
    "    validation_accuracy = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    loss = session.run( cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    \n",
    "    prt = 'Loss: {:.4f} Accuracy: {:.4f}'\n",
    "    print(prt.format(loss, validation_accuracy, prec=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数\n",
    "\n",
    "调试以下超参数：\n",
    "* 设置 `epochs` 表示神经网络停止学习或开始过拟合的迭代次数\n",
    "* 设置 `batch_size`，表示机器内存允许的部分最大体积。大部分人设为以下常见内存大小：\n",
    "\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* 设置 `keep_probability` 表示使用丢弃时保留节点的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在单个 CIFAR-10 部分上训练\n",
    "\n",
    "我们先用单个部分，而不是用所有的 CIFAR-10 批次训练神经网络。这样可以节省时间，并对模型进行迭代，以提高准确率。最终验证准确率达到 50% 或以上之后，在下一部分对所有数据运行模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2983 Accuracy: 0.1052\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.3019 Accuracy: 0.1010\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.2950 Accuracy: 0.1258\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 2.2787 Accuracy: 0.1510\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 2.2601 Accuracy: 0.1592\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 2.2481 Accuracy: 0.1556\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 2.2328 Accuracy: 0.1614\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 2.1974 Accuracy: 0.1950\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 2.1336 Accuracy: 0.2274\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 2.0589 Accuracy: 0.2510\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.9907 Accuracy: 0.3046\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 2.0030 Accuracy: 0.3036\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.9698 Accuracy: 0.3454\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.9381 Accuracy: 0.3614\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.9321 Accuracy: 0.3576\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.9110 Accuracy: 0.3924\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.8847 Accuracy: 0.3914\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.8655 Accuracy: 0.4000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.7937 Accuracy: 0.4150\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.8148 Accuracy: 0.4054\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.8180 Accuracy: 0.4088\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.7589 Accuracy: 0.4220\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.7052 Accuracy: 0.4270\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.6943 Accuracy: 0.4394\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.7342 Accuracy: 0.4196\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.6755 Accuracy: 0.4378\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.7083 Accuracy: 0.4226\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.6642 Accuracy: 0.4306\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.6127 Accuracy: 0.4432\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.6448 Accuracy: 0.4346\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.7516 Accuracy: 0.4092\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.8120 Accuracy: 0.3852\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.6668 Accuracy: 0.4274\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.6681 Accuracy: 0.4178\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.4836 Accuracy: 0.4710\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.5404 Accuracy: 0.4686\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.5020 Accuracy: 0.4682\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 1.6325 Accuracy: 0.4268\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 1.5342 Accuracy: 0.4504\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 1.4532 Accuracy: 0.4696\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 1.4693 Accuracy: 0.4648\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 1.4926 Accuracy: 0.4594\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 1.4819 Accuracy: 0.4456\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 1.3472 Accuracy: 0.4768\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 1.4583 Accuracy: 0.4568\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 1.3864 Accuracy: 0.4916\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 1.4450 Accuracy: 0.4776\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 1.4338 Accuracy: 0.4794\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 1.4163 Accuracy: 0.4852\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 1.5015 Accuracy: 0.4518\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 1.2733 Accuracy: 0.4982\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 1.2747 Accuracy: 0.5010\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 1.2742 Accuracy: 0.5102\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 1.1779 Accuracy: 0.5264\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 1.2011 Accuracy: 0.5122\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 1.1687 Accuracy: 0.5296\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 1.2115 Accuracy: 0.5148\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 1.1978 Accuracy: 0.5310\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 1.0567 Accuracy: 0.5438\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 1.0710 Accuracy: 0.5416\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 1.3034 Accuracy: 0.5246\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 1.2312 Accuracy: 0.5232\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.8969 Accuracy: 0.5768\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 1.1836 Accuracy: 0.5162\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.9282 Accuracy: 0.5576\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.9694 Accuracy: 0.5752\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.9271 Accuracy: 0.5564\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 1.0297 Accuracy: 0.5514\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.9722 Accuracy: 0.5600\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.8621 Accuracy: 0.5584\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.7735 Accuracy: 0.5928\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.7675 Accuracy: 0.5842\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.7573 Accuracy: 0.5896\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.8836 Accuracy: 0.5774\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.7967 Accuracy: 0.5888\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.8046 Accuracy: 0.5856\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.8143 Accuracy: 0.5876\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.6007 Accuracy: 0.6284\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.7138 Accuracy: 0.6140\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.7523 Accuracy: 0.5996\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.5827 Accuracy: 0.6090\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.5288 Accuracy: 0.6216\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.6867 Accuracy: 0.5948\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.5612 Accuracy: 0.6026\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.5291 Accuracy: 0.6164\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.5034 Accuracy: 0.6268\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.4220 Accuracy: 0.6418\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.4642 Accuracy: 0.6306\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.4608 Accuracy: 0.6152\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.4452 Accuracy: 0.6202\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.4382 Accuracy: 0.6230\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.4110 Accuracy: 0.6286\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.3510 Accuracy: 0.6536\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.5068 Accuracy: 0.6160\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.3317 Accuracy: 0.6320\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.3361 Accuracy: 0.6398\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.3171 Accuracy: 0.6428\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.3584 Accuracy: 0.6360\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.3089 Accuracy: 0.6298\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.2543 Accuracy: 0.6514\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 0.3009 Accuracy: 0.6462\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 0.2655 Accuracy: 0.6338\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 0.3240 Accuracy: 0.6238\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 0.2384 Accuracy: 0.6342\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 0.2554 Accuracy: 0.6394\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 0.2115 Accuracy: 0.6658\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 0.1919 Accuracy: 0.6430\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 0.1703 Accuracy: 0.6618\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 0.1701 Accuracy: 0.6584\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 0.2421 Accuracy: 0.6316\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 0.1909 Accuracy: 0.6492\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 0.1853 Accuracy: 0.6550\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 0.1865 Accuracy: 0.6508\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 0.1740 Accuracy: 0.6566\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 0.1679 Accuracy: 0.6458\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 0.1648 Accuracy: 0.6430\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 0.1855 Accuracy: 0.6450\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 0.1670 Accuracy: 0.6528\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 0.1661 Accuracy: 0.6412\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 0.1051 Accuracy: 0.6566\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 0.0815 Accuracy: 0.6694\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 0.0835 Accuracy: 0.6744\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 0.0970 Accuracy: 0.6672\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 0.0846 Accuracy: 0.6546\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 0.0916 Accuracy: 0.6660\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 0.0668 Accuracy: 0.6756\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 0.0661 Accuracy: 0.6726\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 0.0693 Accuracy: 0.6738\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 0.0782 Accuracy: 0.6628\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 0.0725 Accuracy: 0.6670\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 0.0957 Accuracy: 0.6616\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 0.0959 Accuracy: 0.6536\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 0.0861 Accuracy: 0.6546\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 0.0524 Accuracy: 0.6670\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 0.0457 Accuracy: 0.6680\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 0.0494 Accuracy: 0.6748\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 0.0328 Accuracy: 0.6736\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 0.0367 Accuracy: 0.6800\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 0.0377 Accuracy: 0.6724\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 0.0390 Accuracy: 0.6774\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 0.0469 Accuracy: 0.6624\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 0.0420 Accuracy: 0.6754\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 0.0444 Accuracy: 0.6684\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 0.0311 Accuracy: 0.6678\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 0.0244 Accuracy: 0.6870\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 0.0256 Accuracy: 0.6916\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 0.0382 Accuracy: 0.6788\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 0.0364 Accuracy: 0.6724\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 0.0314 Accuracy: 0.6722\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 0.0238 Accuracy: 0.6872\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss: 0.0274 Accuracy: 0.6614\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss: 0.0197 Accuracy: 0.6724\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss: 0.0162 Accuracy: 0.6890\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss: 0.0305 Accuracy: 0.6794\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss: 0.0187 Accuracy: 0.6912\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss: 0.0214 Accuracy: 0.6702\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss: 0.0166 Accuracy: 0.6830\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss: 0.0180 Accuracy: 0.6758\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss: 0.0201 Accuracy: 0.6864\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss: 0.0141 Accuracy: 0.6844\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss: 0.0152 Accuracy: 0.6800\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss: 0.0171 Accuracy: 0.6790\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss: 0.0108 Accuracy: 0.6830\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss: 0.0092 Accuracy: 0.6878\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss: 0.0300 Accuracy: 0.6662\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss: 0.0240 Accuracy: 0.6798\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss: 0.0102 Accuracy: 0.6732\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss: 0.0129 Accuracy: 0.6688\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss: 0.0084 Accuracy: 0.6810\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss: 0.0082 Accuracy: 0.6846\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss: 0.0064 Accuracy: 0.6870\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss: 0.0122 Accuracy: 0.6616\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss: 0.0074 Accuracy: 0.6762\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss: 0.0214 Accuracy: 0.6730\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss: 0.0078 Accuracy: 0.6768\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss: 0.0061 Accuracy: 0.6896\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss: 0.0217 Accuracy: 0.6620\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss: 0.0059 Accuracy: 0.6882\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss: 0.0048 Accuracy: 0.6914\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss: 0.0081 Accuracy: 0.6790\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss: 0.0058 Accuracy: 0.6862\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss: 0.0070 Accuracy: 0.6808\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss: 0.0163 Accuracy: 0.6722\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss: 0.0093 Accuracy: 0.6856\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss: 0.0096 Accuracy: 0.6850\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss: 0.0100 Accuracy: 0.6802\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss: 0.0064 Accuracy: 0.6758\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss: 0.0148 Accuracy: 0.6520\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss: 0.0065 Accuracy: 0.6796\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss: 0.0063 Accuracy: 0.6796\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss: 0.0041 Accuracy: 0.6740\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss: 0.0039 Accuracy: 0.6714\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss: 0.0027 Accuracy: 0.6938\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss: 0.0022 Accuracy: 0.6938\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss: 0.0029 Accuracy: 0.6778\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss: 0.0093 Accuracy: 0.6772\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss: 0.0029 Accuracy: 0.6734\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss: 0.0023 Accuracy: 0.6838\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss: 0.0034 Accuracy: 0.6754\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss: 0.0028 Accuracy: 0.6836\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完全训练模型\n",
    "\n",
    "现在，单个 CIFAR-10 部分的准确率已经不错了，试试所有五个部分吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2990 Accuracy: 0.1242\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 2.2989 Accuracy: 0.0984\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 2.2903 Accuracy: 0.1134\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 2.2772 Accuracy: 0.1192\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 2.2432 Accuracy: 0.1762\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.1821 Accuracy: 0.2398\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 2.0755 Accuracy: 0.2804\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.9589 Accuracy: 0.2936\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.9475 Accuracy: 0.3336\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.9736 Accuracy: 0.3324\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.0157 Accuracy: 0.3342\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 1.8046 Accuracy: 0.3512\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 1.7284 Accuracy: 0.3564\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.7664 Accuracy: 0.3980\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.7433 Accuracy: 0.3826\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.8219 Accuracy: 0.4198\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.6349 Accuracy: 0.4172\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.5965 Accuracy: 0.4204\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.6699 Accuracy: 0.4382\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.6740 Accuracy: 0.4078\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.7249 Accuracy: 0.4488\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.4835 Accuracy: 0.4432\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.4859 Accuracy: 0.4434\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.6320 Accuracy: 0.4500\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 1.5858 Accuracy: 0.4222\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.6674 Accuracy: 0.4660\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.5061 Accuracy: 0.4328\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 1.4265 Accuracy: 0.4772\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 1.5376 Accuracy: 0.4732\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 1.4722 Accuracy: 0.4830\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.5917 Accuracy: 0.4764\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 1.4341 Accuracy: 0.4738\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 1.3711 Accuracy: 0.4870\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 1.4752 Accuracy: 0.4874\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 1.4793 Accuracy: 0.4506\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.5289 Accuracy: 0.5098\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 1.4208 Accuracy: 0.5026\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 1.3289 Accuracy: 0.5000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 1.4045 Accuracy: 0.5242\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 1.4499 Accuracy: 0.4628\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.4232 Accuracy: 0.5146\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 1.3775 Accuracy: 0.5368\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 1.3581 Accuracy: 0.5020\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 1.3345 Accuracy: 0.5460\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 1.3752 Accuracy: 0.5066\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.3309 Accuracy: 0.5494\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 1.3696 Accuracy: 0.5344\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 1.2076 Accuracy: 0.5432\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 1.2601 Accuracy: 0.5446\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 1.3157 Accuracy: 0.5092\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.2238 Accuracy: 0.5454\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 1.4382 Accuracy: 0.5516\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 1.2283 Accuracy: 0.5364\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 1.1790 Accuracy: 0.5680\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 1.2405 Accuracy: 0.5320\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.1388 Accuracy: 0.5820\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 1.3224 Accuracy: 0.5634\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 1.1503 Accuracy: 0.5740\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 1.1259 Accuracy: 0.5864\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 1.2610 Accuracy: 0.5458\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.1514 Accuracy: 0.5936\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 1.2045 Accuracy: 0.6064\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 1.1842 Accuracy: 0.5526\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 1.0902 Accuracy: 0.5976\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 1.2523 Accuracy: 0.5404\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.0472 Accuracy: 0.6164\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 1.1703 Accuracy: 0.6236\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 0.9812 Accuracy: 0.6112\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 1.0638 Accuracy: 0.5832\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 1.1103 Accuracy: 0.5834\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.0332 Accuracy: 0.6114\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 1.1341 Accuracy: 0.6414\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 1.0505 Accuracy: 0.5838\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 0.9442 Accuracy: 0.6464\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 1.1141 Accuracy: 0.5910\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.0301 Accuracy: 0.6044\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 1.1813 Accuracy: 0.6204\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 1.0379 Accuracy: 0.5800\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 0.9038 Accuracy: 0.6382\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 1.1423 Accuracy: 0.6112\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 0.9734 Accuracy: 0.6424\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 1.0955 Accuracy: 0.6250\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 0.9875 Accuracy: 0.5926\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 0.8259 Accuracy: 0.6594\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 1.0233 Accuracy: 0.6412\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 0.8530 Accuracy: 0.6746\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 1.0345 Accuracy: 0.6548\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 0.8824 Accuracy: 0.6372\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 0.8165 Accuracy: 0.6660\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 0.9477 Accuracy: 0.6742\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 0.8394 Accuracy: 0.6670\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 1.0045 Accuracy: 0.6578\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 0.7900 Accuracy: 0.6604\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 0.7782 Accuracy: 0.6710\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 0.9517 Accuracy: 0.6470\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 0.7991 Accuracy: 0.6802\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 1.0704 Accuracy: 0.6352\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 0.8044 Accuracy: 0.6554\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 0.8120 Accuracy: 0.6718\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 0.9089 Accuracy: 0.6626\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 0.7858 Accuracy: 0.6850\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 1.0655 Accuracy: 0.6426\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 0.7813 Accuracy: 0.6518\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 0.8061 Accuracy: 0.6624\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 0.8049 Accuracy: 0.6740\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 0.7169 Accuracy: 0.6954\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 0.9672 Accuracy: 0.6764\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 0.7923 Accuracy: 0.6416\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 0.7833 Accuracy: 0.6720\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 0.8444 Accuracy: 0.6804\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.7841 Accuracy: 0.6918\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 0.8397 Accuracy: 0.7058\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 0.7241 Accuracy: 0.6434\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 0.7098 Accuracy: 0.6898\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 0.8419 Accuracy: 0.6642\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.7428 Accuracy: 0.6838\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 0.7919 Accuracy: 0.6878\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 0.6420 Accuracy: 0.6858\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 0.6830 Accuracy: 0.6962\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 0.8711 Accuracy: 0.6720\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.6751 Accuracy: 0.7030\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 0.7409 Accuracy: 0.6998\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 0.5714 Accuracy: 0.6954\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 0.5789 Accuracy: 0.7102\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 0.6612 Accuracy: 0.7132\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.6890 Accuracy: 0.6960\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 0.7506 Accuracy: 0.7046\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 0.7047 Accuracy: 0.6746\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 0.7435 Accuracy: 0.6876\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 0.7175 Accuracy: 0.6966\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.6820 Accuracy: 0.7110\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 0.7786 Accuracy: 0.6828\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 0.5677 Accuracy: 0.6962\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 0.5545 Accuracy: 0.7230\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 0.7726 Accuracy: 0.7030\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.7231 Accuracy: 0.7082\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 0.7387 Accuracy: 0.6892\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 0.5264 Accuracy: 0.6916\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 0.5427 Accuracy: 0.7216\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 0.6609 Accuracy: 0.7050\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.6431 Accuracy: 0.7190\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 0.7013 Accuracy: 0.7066\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 0.4793 Accuracy: 0.7118\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.6576 Accuracy: 0.6996\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.7519 Accuracy: 0.6874\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.6193 Accuracy: 0.7420\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 0.6538 Accuracy: 0.7138\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 0.4199 Accuracy: 0.7212\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.6472 Accuracy: 0.7076\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.6747 Accuracy: 0.6896\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.5582 Accuracy: 0.7334\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 0.6682 Accuracy: 0.6976\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 0.5242 Accuracy: 0.7138\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.5034 Accuracy: 0.7270\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.5787 Accuracy: 0.7188\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.5440 Accuracy: 0.7352\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.6663 Accuracy: 0.6984\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.4486 Accuracy: 0.7248\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.5507 Accuracy: 0.7260\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.5716 Accuracy: 0.7218\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.4697 Accuracy: 0.7300\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.5284 Accuracy: 0.7210\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 0.5077 Accuracy: 0.7036\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.5529 Accuracy: 0.7162\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.5547 Accuracy: 0.7324\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.4196 Accuracy: 0.7514\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.5479 Accuracy: 0.7204\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.4248 Accuracy: 0.7364\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.4809 Accuracy: 0.7402\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.5629 Accuracy: 0.7230\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.5213 Accuracy: 0.7484\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.7084 Accuracy: 0.6712\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.3471 Accuracy: 0.7396\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.4446 Accuracy: 0.7326\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.4609 Accuracy: 0.7348\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.4922 Accuracy: 0.7350\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 0.6410 Accuracy: 0.7082\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 0.3765 Accuracy: 0.7278\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 0.4533 Accuracy: 0.7218\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 0.4645 Accuracy: 0.7250\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.4627 Accuracy: 0.7498\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 0.5271 Accuracy: 0.7338\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 0.3728 Accuracy: 0.7374\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 0.4523 Accuracy: 0.7340\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 0.4630 Accuracy: 0.7520\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.5189 Accuracy: 0.7442\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 0.5561 Accuracy: 0.7230\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.2987 Accuracy: 0.7396\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 0.4000 Accuracy: 0.7484\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 0.4616 Accuracy: 0.7384\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.4557 Accuracy: 0.7560\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 0.5801 Accuracy: 0.7286\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.3125 Accuracy: 0.7412\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 0.4242 Accuracy: 0.7276\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 0.4778 Accuracy: 0.7144\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.4739 Accuracy: 0.7380\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 0.4849 Accuracy: 0.7390\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.3120 Accuracy: 0.7432\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 0.4082 Accuracy: 0.7340\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 0.5077 Accuracy: 0.7244\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.4157 Accuracy: 0.7522\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 0.4557 Accuracy: 0.7448\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.3861 Accuracy: 0.7230\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 0.3625 Accuracy: 0.7510\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 0.4289 Accuracy: 0.7454\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.4259 Accuracy: 0.7476\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 0.4318 Accuracy: 0.7506\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.3366 Accuracy: 0.7372\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 0.4306 Accuracy: 0.7356\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 0.3897 Accuracy: 0.7408\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.3964 Accuracy: 0.7608\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 0.4090 Accuracy: 0.7450\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.2613 Accuracy: 0.7630\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 0.4524 Accuracy: 0.7304\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 0.3883 Accuracy: 0.7356\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.3773 Accuracy: 0.7708\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 0.4500 Accuracy: 0.7426\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.2481 Accuracy: 0.7578\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 0.3320 Accuracy: 0.7530\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 0.4072 Accuracy: 0.7472\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.3763 Accuracy: 0.7614\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 0.4845 Accuracy: 0.7414\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.2826 Accuracy: 0.7508\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 0.3281 Accuracy: 0.7436\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 0.3487 Accuracy: 0.7534\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.4031 Accuracy: 0.7546\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 0.3633 Accuracy: 0.7678\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.2409 Accuracy: 0.7632\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 0.3666 Accuracy: 0.7542\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 0.3941 Accuracy: 0.7300\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.3484 Accuracy: 0.7542\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.4104 Accuracy: 0.7450\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.2377 Accuracy: 0.7626\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 0.3378 Accuracy: 0.7440\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.3510 Accuracy: 0.7476\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.4207 Accuracy: 0.7376\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.4389 Accuracy: 0.7316\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.3238 Accuracy: 0.7228\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 0.3445 Accuracy: 0.7444\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.3402 Accuracy: 0.7372\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.3516 Accuracy: 0.7594\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.3818 Accuracy: 0.7540\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.2065 Accuracy: 0.7682\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 0.3164 Accuracy: 0.7456\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.2689 Accuracy: 0.7660\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.3109 Accuracy: 0.7694\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.4700 Accuracy: 0.7272\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 0.2431 Accuracy: 0.7674\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.2287 Accuracy: 0.7774\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.3236 Accuracy: 0.7576\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.3493 Accuracy: 0.7778\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.3794 Accuracy: 0.7658\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.2293 Accuracy: 0.7552\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.2817 Accuracy: 0.7592\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.3450 Accuracy: 0.7548\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.3077 Accuracy: 0.7844\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.3874 Accuracy: 0.7544\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 0.2020 Accuracy: 0.7696\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.2275 Accuracy: 0.7654\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.2890 Accuracy: 0.7626\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.2955 Accuracy: 0.7866\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.4292 Accuracy: 0.7440\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.1879 Accuracy: 0.7640\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.2703 Accuracy: 0.7720\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.3677 Accuracy: 0.7470\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.2827 Accuracy: 0.7698\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.2753 Accuracy: 0.7724\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.1960 Accuracy: 0.7746\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.2482 Accuracy: 0.7790\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.2988 Accuracy: 0.7722\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.2741 Accuracy: 0.7806\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.2970 Accuracy: 0.7642\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 0.2392 Accuracy: 0.7692\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.1716 Accuracy: 0.7882\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.2274 Accuracy: 0.7718\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.2951 Accuracy: 0.7694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.3403 Accuracy: 0.7468\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.1810 Accuracy: 0.7836\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.2145 Accuracy: 0.7850\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.3042 Accuracy: 0.7594\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.2884 Accuracy: 0.7658\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.2674 Accuracy: 0.7804\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.1921 Accuracy: 0.7710\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.2170 Accuracy: 0.7668\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.2518 Accuracy: 0.7892\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.2383 Accuracy: 0.7792\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.2831 Accuracy: 0.7554\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.2315 Accuracy: 0.7632\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.2486 Accuracy: 0.7730\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.2755 Accuracy: 0.7628\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.2333 Accuracy: 0.7958\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 0.2962 Accuracy: 0.7600\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.1798 Accuracy: 0.7806\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 0.2057 Accuracy: 0.7830\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 0.3137 Accuracy: 0.7486\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.2201 Accuracy: 0.7898\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 0.2238 Accuracy: 0.7750\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.1981 Accuracy: 0.7818\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.2792 Accuracy: 0.7504\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.2755 Accuracy: 0.7642\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.2574 Accuracy: 0.7784\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.2490 Accuracy: 0.7576\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.1884 Accuracy: 0.7766\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 0.1939 Accuracy: 0.7762\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.2356 Accuracy: 0.7808\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.2343 Accuracy: 0.7926\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.3334 Accuracy: 0.7350\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.1817 Accuracy: 0.7824\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.2095 Accuracy: 0.7612\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.2359 Accuracy: 0.7862\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.2654 Accuracy: 0.7874\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.3100 Accuracy: 0.7570\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.2000 Accuracy: 0.7798\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.2135 Accuracy: 0.7792\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.3026 Accuracy: 0.7548\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.2528 Accuracy: 0.7826\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.2637 Accuracy: 0.7676\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 0.2186 Accuracy: 0.7760\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.1635 Accuracy: 0.7900\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.2607 Accuracy: 0.7766\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.2484 Accuracy: 0.7794\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 0.1882 Accuracy: 0.7842\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 0.1645 Accuracy: 0.7824\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.1705 Accuracy: 0.7884\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 0.2601 Accuracy: 0.7752\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.2260 Accuracy: 0.7830\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.2603 Accuracy: 0.7730\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 0.1915 Accuracy: 0.7780\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.3277 Accuracy: 0.7510\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.2697 Accuracy: 0.7752\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.2551 Accuracy: 0.7844\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.2440 Accuracy: 0.7720\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.1791 Accuracy: 0.7800\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.1722 Accuracy: 0.7726\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.2186 Accuracy: 0.7840\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.2175 Accuracy: 0.7900\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.2495 Accuracy: 0.7510\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 0.1426 Accuracy: 0.7798\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.2684 Accuracy: 0.7616\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.1852 Accuracy: 0.7928\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.2183 Accuracy: 0.7860\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.3440 Accuracy: 0.7412\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 0.1636 Accuracy: 0.7816\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.1744 Accuracy: 0.7842\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.2064 Accuracy: 0.7742\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.2010 Accuracy: 0.7926\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 0.2566 Accuracy: 0.7720\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.1592 Accuracy: 0.7802\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 0.1877 Accuracy: 0.7862\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 0.1922 Accuracy: 0.7680\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.1795 Accuracy: 0.7926\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 0.2239 Accuracy: 0.7724\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 0.1505 Accuracy: 0.7886\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 0.2080 Accuracy: 0.7698\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 0.1783 Accuracy: 0.7746\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.2619 Accuracy: 0.7806\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 0.1514 Accuracy: 0.7862\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 0.1488 Accuracy: 0.7874\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 0.2200 Accuracy: 0.7806\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 0.1892 Accuracy: 0.7672\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.2338 Accuracy: 0.7884\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 0.1864 Accuracy: 0.7856\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 0.1493 Accuracy: 0.7812\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 0.1876 Accuracy: 0.7778\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 0.1603 Accuracy: 0.7786\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.1754 Accuracy: 0.8002\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 0.3595 Accuracy: 0.7656\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 0.1570 Accuracy: 0.7756\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 0.2290 Accuracy: 0.7806\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 0.2107 Accuracy: 0.7598\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.1622 Accuracy: 0.8036\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 0.2064 Accuracy: 0.7762\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 0.1033 Accuracy: 0.7940\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 0.1807 Accuracy: 0.7758\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 0.1870 Accuracy: 0.7864\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.1635 Accuracy: 0.7968\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 0.2108 Accuracy: 0.7980\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 0.1323 Accuracy: 0.7894\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 0.2962 Accuracy: 0.7454\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 0.1669 Accuracy: 0.7772\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.1335 Accuracy: 0.7950\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 0.2360 Accuracy: 0.7780\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 0.1434 Accuracy: 0.7880\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 0.1853 Accuracy: 0.7884\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 0.1815 Accuracy: 0.7968\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.1258 Accuracy: 0.7974\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 0.2947 Accuracy: 0.7640\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 0.1014 Accuracy: 0.7970\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 0.1411 Accuracy: 0.7940\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 0.1531 Accuracy: 0.7888\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.1449 Accuracy: 0.8040\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 0.2323 Accuracy: 0.7800\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 0.1313 Accuracy: 0.7974\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 0.1834 Accuracy: 0.7778\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 0.1888 Accuracy: 0.7722\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.1487 Accuracy: 0.7974\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 0.1959 Accuracy: 0.7980\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 0.0917 Accuracy: 0.7958\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 0.1517 Accuracy: 0.7900\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 0.1888 Accuracy: 0.7684\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.1392 Accuracy: 0.7970\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 0.2158 Accuracy: 0.7686\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 0.0954 Accuracy: 0.7908\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 0.1366 Accuracy: 0.7808\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 0.1450 Accuracy: 0.7726\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.1794 Accuracy: 0.7808\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 0.2211 Accuracy: 0.7808\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 0.1187 Accuracy: 0.7898\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 0.1217 Accuracy: 0.7854\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 0.1527 Accuracy: 0.7974\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.1929 Accuracy: 0.7808\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 0.2924 Accuracy: 0.7734\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 0.1117 Accuracy: 0.7836\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 0.1438 Accuracy: 0.7788\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 0.1404 Accuracy: 0.7820\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.1202 Accuracy: 0.7968\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 0.1920 Accuracy: 0.7740\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 0.1085 Accuracy: 0.7820\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 0.2698 Accuracy: 0.7524\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 0.1950 Accuracy: 0.7684\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.1272 Accuracy: 0.7976\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 0.1620 Accuracy: 0.7878\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 0.1265 Accuracy: 0.7946\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 0.1759 Accuracy: 0.7804\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 0.1856 Accuracy: 0.7720\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.1135 Accuracy: 0.7922\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 0.1171 Accuracy: 0.7912\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 0.1472 Accuracy: 0.7764\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 0.1904 Accuracy: 0.7844\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 0.1731 Accuracy: 0.7856\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.1536 Accuracy: 0.7910\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 0.3383 Accuracy: 0.7642\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 0.1288 Accuracy: 0.7812\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 0.1544 Accuracy: 0.7934\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 0.1477 Accuracy: 0.7800\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.1224 Accuracy: 0.7976\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 0.1642 Accuracy: 0.7954\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 0.1710 Accuracy: 0.7674\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 0.1429 Accuracy: 0.7988\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 0.1591 Accuracy: 0.7764\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.1344 Accuracy: 0.7978\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 0.1679 Accuracy: 0.7954\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 0.1350 Accuracy: 0.7762\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 0.1534 Accuracy: 0.7892\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 0.1498 Accuracy: 0.7732\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.0992 Accuracy: 0.8042\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 0.1749 Accuracy: 0.7946\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 0.1035 Accuracy: 0.7924\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 0.1504 Accuracy: 0.7994\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 0.1795 Accuracy: 0.7638\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.1520 Accuracy: 0.7862\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 0.2888 Accuracy: 0.7520\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 0.1588 Accuracy: 0.7558\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 0.1664 Accuracy: 0.7760\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 0.1491 Accuracy: 0.7670\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.0892 Accuracy: 0.8040\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 0.2005 Accuracy: 0.7770\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 0.1064 Accuracy: 0.7818\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 0.1695 Accuracy: 0.7834\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 0.1903 Accuracy: 0.7612\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.1117 Accuracy: 0.8054\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 0.1942 Accuracy: 0.8026\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 0.1192 Accuracy: 0.7930\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 0.0932 Accuracy: 0.8006\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 0.1382 Accuracy: 0.7816\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.0974 Accuracy: 0.8028\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 0.2104 Accuracy: 0.7888\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 0.0862 Accuracy: 0.7926\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 0.1150 Accuracy: 0.7972\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 0.1109 Accuracy: 0.7950\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.0732 Accuracy: 0.8030\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 0.1463 Accuracy: 0.7982\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 0.1428 Accuracy: 0.7850\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 0.1353 Accuracy: 0.7928\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 0.1090 Accuracy: 0.7924\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.0720 Accuracy: 0.8048\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 0.1102 Accuracy: 0.7990\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 0.0865 Accuracy: 0.8028\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 0.1282 Accuracy: 0.7920\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 0.1164 Accuracy: 0.7912\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.1014 Accuracy: 0.7912\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 0.1291 Accuracy: 0.7958\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 0.0814 Accuracy: 0.8014\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 0.1019 Accuracy: 0.7980\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 0.1268 Accuracy: 0.7920\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.0837 Accuracy: 0.7988\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 0.1363 Accuracy: 0.7896\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 0.0744 Accuracy: 0.8028\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 0.1629 Accuracy: 0.7948\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 0.1063 Accuracy: 0.7902\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.1060 Accuracy: 0.7936\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 0.1079 Accuracy: 0.8010\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 0.0980 Accuracy: 0.7948\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 0.0954 Accuracy: 0.7986\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 0.1054 Accuracy: 0.7884\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.1235 Accuracy: 0.7900\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 0.2820 Accuracy: 0.7626\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 0.0753 Accuracy: 0.7966\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 0.1661 Accuracy: 0.7916\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 0.0861 Accuracy: 0.7882\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 0.0907 Accuracy: 0.8092\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss: 0.1732 Accuracy: 0.7762\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss: 0.0798 Accuracy: 0.7942\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss: 0.1809 Accuracy: 0.7762\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss: 0.2050 Accuracy: 0.7522\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 0.0810 Accuracy: 0.8052\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss: 0.1305 Accuracy: 0.7838\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss: 0.0809 Accuracy: 0.8042\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss: 0.1608 Accuracy: 0.7862\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss: 0.1086 Accuracy: 0.7842\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 0.1055 Accuracy: 0.7996\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss: 0.1220 Accuracy: 0.7960\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss: 0.1136 Accuracy: 0.7860\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss: 0.1569 Accuracy: 0.7962\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss: 0.1045 Accuracy: 0.7888\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 0.1448 Accuracy: 0.7954\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss: 0.1258 Accuracy: 0.7974\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss: 0.0888 Accuracy: 0.7890\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss: 0.1362 Accuracy: 0.7998\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss: 0.0801 Accuracy: 0.7922\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 0.0985 Accuracy: 0.8030\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss: 0.1245 Accuracy: 0.7868\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss: 0.1200 Accuracy: 0.7858\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss: 0.1354 Accuracy: 0.7940\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss: 0.0956 Accuracy: 0.7796\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 0.1035 Accuracy: 0.8052\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss: 0.0942 Accuracy: 0.8114\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss: 0.0724 Accuracy: 0.8052\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss: 0.0935 Accuracy: 0.8068\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss: 0.1139 Accuracy: 0.7810\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 0.1066 Accuracy: 0.8042\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss: 0.2231 Accuracy: 0.7796\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss: 0.0746 Accuracy: 0.8014\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss: 0.0821 Accuracy: 0.7928\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss: 0.1028 Accuracy: 0.7836\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 0.0706 Accuracy: 0.8090\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss: 0.1134 Accuracy: 0.8004\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss: 0.1207 Accuracy: 0.7826\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss: 0.0979 Accuracy: 0.8036\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss: 0.0770 Accuracy: 0.7872\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 0.0838 Accuracy: 0.7982\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss: 0.0967 Accuracy: 0.7966\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss: 0.1000 Accuracy: 0.7956\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss: 0.1234 Accuracy: 0.7898\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss: 0.0972 Accuracy: 0.7880\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 0.0765 Accuracy: 0.8034\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss: 0.1914 Accuracy: 0.7880\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss: 0.0967 Accuracy: 0.7914\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss: 0.0892 Accuracy: 0.8016\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss: 0.1224 Accuracy: 0.7656\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 0.0853 Accuracy: 0.8048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111, CIFAR-10 Batch 2:  Loss: 0.1268 Accuracy: 0.7990\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss: 0.0800 Accuracy: 0.7932\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss: 0.1123 Accuracy: 0.7942\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss: 0.0782 Accuracy: 0.7964\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 0.0619 Accuracy: 0.7990\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss: 0.0931 Accuracy: 0.7870\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss: 0.0805 Accuracy: 0.7992\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss: 0.0866 Accuracy: 0.8088\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss: 0.0831 Accuracy: 0.8022\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 0.0708 Accuracy: 0.8118\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss: 0.1265 Accuracy: 0.7814\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss: 0.0571 Accuracy: 0.8064\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss: 0.0709 Accuracy: 0.8070\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss: 0.0980 Accuracy: 0.7852\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 0.1238 Accuracy: 0.8064\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss: 0.1130 Accuracy: 0.8064\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss: 0.0705 Accuracy: 0.8012\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss: 0.1281 Accuracy: 0.8006\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss: 0.1003 Accuracy: 0.7940\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 0.2878 Accuracy: 0.7700\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss: 0.1255 Accuracy: 0.8024\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss: 0.0637 Accuracy: 0.8006\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss: 0.0744 Accuracy: 0.8072\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss: 0.1021 Accuracy: 0.7868\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 0.1043 Accuracy: 0.7908\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss: 0.1340 Accuracy: 0.7984\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss: 0.0839 Accuracy: 0.7972\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss: 0.0698 Accuracy: 0.8160\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss: 0.0765 Accuracy: 0.7948\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 0.0484 Accuracy: 0.7932\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss: 0.0914 Accuracy: 0.8020\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss: 0.0499 Accuracy: 0.8050\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss: 0.1053 Accuracy: 0.7968\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss: 0.0557 Accuracy: 0.7914\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 0.0575 Accuracy: 0.7968\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss: 0.1275 Accuracy: 0.7982\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss: 0.0436 Accuracy: 0.8094\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss: 0.0990 Accuracy: 0.7894\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss: 0.0818 Accuracy: 0.7902\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 0.1306 Accuracy: 0.7936\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss: 0.0768 Accuracy: 0.8054\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss: 0.0710 Accuracy: 0.7994\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss: 0.0903 Accuracy: 0.8016\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss: 0.0859 Accuracy: 0.7890\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 0.0761 Accuracy: 0.7934\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss: 0.0918 Accuracy: 0.7994\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss: 0.0624 Accuracy: 0.8030\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss: 0.0677 Accuracy: 0.8062\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss: 0.0746 Accuracy: 0.7896\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 0.0528 Accuracy: 0.8068\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss: 0.0982 Accuracy: 0.7932\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss: 0.0483 Accuracy: 0.8030\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss: 0.0859 Accuracy: 0.8108\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss: 0.0608 Accuracy: 0.8086\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 0.0497 Accuracy: 0.8060\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss: 0.1312 Accuracy: 0.7942\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss: 0.0721 Accuracy: 0.7994\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss: 0.0655 Accuracy: 0.8074\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss: 0.0647 Accuracy: 0.7952\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 0.0636 Accuracy: 0.8078\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss: 0.0940 Accuracy: 0.7914\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss: 0.0539 Accuracy: 0.8104\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss: 0.0842 Accuracy: 0.7950\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss: 0.0725 Accuracy: 0.8054\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 0.0639 Accuracy: 0.8040\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss: 0.0823 Accuracy: 0.7966\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss: 0.0572 Accuracy: 0.8064\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss: 0.0935 Accuracy: 0.8010\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss: 0.1143 Accuracy: 0.7756\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 0.0741 Accuracy: 0.8042\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss: 0.0998 Accuracy: 0.7998\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss: 0.0774 Accuracy: 0.7944\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss: 0.0793 Accuracy: 0.8146\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss: 0.0812 Accuracy: 0.7932\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 0.0457 Accuracy: 0.8168\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss: 0.0890 Accuracy: 0.8078\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss: 0.0453 Accuracy: 0.8024\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss: 0.1018 Accuracy: 0.8046\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss: 0.0660 Accuracy: 0.7948\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 0.0463 Accuracy: 0.8184\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss: 0.1228 Accuracy: 0.7766\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss: 0.0758 Accuracy: 0.7976\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss: 0.0815 Accuracy: 0.8132\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss: 0.0633 Accuracy: 0.7960\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 0.0562 Accuracy: 0.8028\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss: 0.1388 Accuracy: 0.8016\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss: 0.0515 Accuracy: 0.8068\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss: 0.0778 Accuracy: 0.8140\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss: 0.0619 Accuracy: 0.8032\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 0.0422 Accuracy: 0.8078\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss: 0.0970 Accuracy: 0.7990\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss: 0.0580 Accuracy: 0.8086\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss: 0.1259 Accuracy: 0.8012\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss: 0.0527 Accuracy: 0.8068\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 0.0447 Accuracy: 0.8138\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss: 0.0716 Accuracy: 0.8032\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss: 0.0424 Accuracy: 0.8092\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss: 0.0809 Accuracy: 0.8114\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss: 0.0515 Accuracy: 0.7984\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 0.0368 Accuracy: 0.8068\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss: 0.0560 Accuracy: 0.8088\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss: 0.0432 Accuracy: 0.8112\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss: 0.0620 Accuracy: 0.8098\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss: 0.0594 Accuracy: 0.7976\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 0.0460 Accuracy: 0.8084\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss: 0.1086 Accuracy: 0.7964\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss: 0.0677 Accuracy: 0.8060\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss: 0.1011 Accuracy: 0.7976\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss: 0.0646 Accuracy: 0.7936\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 0.0631 Accuracy: 0.8084\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss: 0.1575 Accuracy: 0.7994\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss: 0.0784 Accuracy: 0.7954\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss: 0.0763 Accuracy: 0.8062\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss: 0.0866 Accuracy: 0.7986\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 0.0462 Accuracy: 0.8110\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss: 0.0976 Accuracy: 0.8018\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss: 0.0602 Accuracy: 0.8034\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss: 0.0624 Accuracy: 0.8060\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss: 0.0848 Accuracy: 0.7984\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 0.0539 Accuracy: 0.8032\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss: 0.0645 Accuracy: 0.8050\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss: 0.0683 Accuracy: 0.8034\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss: 0.0752 Accuracy: 0.8052\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss: 0.0702 Accuracy: 0.8066\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 0.0542 Accuracy: 0.8138\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss: 0.0730 Accuracy: 0.8054\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss: 0.0432 Accuracy: 0.8018\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss: 0.0794 Accuracy: 0.8076\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss: 0.0810 Accuracy: 0.7924\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 0.0509 Accuracy: 0.8056\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss: 0.0994 Accuracy: 0.8044\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss: 0.0461 Accuracy: 0.8074\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss: 0.0689 Accuracy: 0.8052\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss: 0.0655 Accuracy: 0.8058\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 0.0496 Accuracy: 0.8046\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss: 0.0719 Accuracy: 0.8000\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss: 0.0447 Accuracy: 0.8018\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss: 0.1721 Accuracy: 0.7850\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss: 0.0840 Accuracy: 0.7852\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 0.0474 Accuracy: 0.8068\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss: 0.0587 Accuracy: 0.8090\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss: 0.0444 Accuracy: 0.8110\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss: 0.0695 Accuracy: 0.8100\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss: 0.0870 Accuracy: 0.7920\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 0.0350 Accuracy: 0.8120\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss: 0.0534 Accuracy: 0.8074\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss: 0.0488 Accuracy: 0.7934\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss: 0.1470 Accuracy: 0.7920\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss: 0.0812 Accuracy: 0.7972\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 0.0273 Accuracy: 0.8076\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss: 0.1401 Accuracy: 0.8064\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss: 0.0448 Accuracy: 0.8060\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss: 0.0419 Accuracy: 0.8218\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss: 0.0773 Accuracy: 0.7942\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 0.0543 Accuracy: 0.8072\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss: 0.0582 Accuracy: 0.7962\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss: 0.0446 Accuracy: 0.7972\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss: 0.0582 Accuracy: 0.8056\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss: 0.0504 Accuracy: 0.7974\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 0.0473 Accuracy: 0.8120\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss: 0.0419 Accuracy: 0.8152\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss: 0.0410 Accuracy: 0.8034\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss: 0.0578 Accuracy: 0.8118\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss: 0.0564 Accuracy: 0.8034\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 0.0500 Accuracy: 0.8096\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss: 0.0672 Accuracy: 0.8090\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss: 0.0316 Accuracy: 0.7934\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss: 0.1097 Accuracy: 0.7962\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss: 0.0475 Accuracy: 0.8038\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 0.0384 Accuracy: 0.8116\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss: 0.0385 Accuracy: 0.8112\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss: 0.0478 Accuracy: 0.7942\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss: 0.0762 Accuracy: 0.8078\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss: 0.0744 Accuracy: 0.7948\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 0.0454 Accuracy: 0.8106\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss: 0.1388 Accuracy: 0.7846\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss: 0.0469 Accuracy: 0.7962\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss: 0.1026 Accuracy: 0.7982\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss: 0.0414 Accuracy: 0.8042\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 0.0373 Accuracy: 0.8138\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss: 0.0533 Accuracy: 0.8050\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss: 0.0544 Accuracy: 0.8044\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss: 0.0556 Accuracy: 0.8092\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss: 0.0659 Accuracy: 0.7756\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 0.0387 Accuracy: 0.8096\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss: 0.1269 Accuracy: 0.7956\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss: 0.0321 Accuracy: 0.8024\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss: 0.0561 Accuracy: 0.8106\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss: 0.0855 Accuracy: 0.7856\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 0.0350 Accuracy: 0.8176\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss: 0.0860 Accuracy: 0.8074\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss: 0.0536 Accuracy: 0.7920\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss: 0.0524 Accuracy: 0.8170\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss: 0.0388 Accuracy: 0.8056\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 0.0558 Accuracy: 0.7996\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss: 0.0762 Accuracy: 0.8040\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss: 0.0585 Accuracy: 0.8052\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss: 0.0829 Accuracy: 0.8072\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss: 0.0522 Accuracy: 0.7970\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss: 0.0321 Accuracy: 0.8190\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss: 0.1268 Accuracy: 0.7994\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss: 0.0326 Accuracy: 0.8092\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss: 0.0969 Accuracy: 0.8008\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss: 0.0626 Accuracy: 0.7790\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss: 0.0243 Accuracy: 0.8166\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss: 0.0886 Accuracy: 0.7914\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss: 0.0380 Accuracy: 0.7924\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss: 0.0602 Accuracy: 0.8044\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss: 0.0723 Accuracy: 0.7802\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss: 0.0234 Accuracy: 0.8212\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss: 0.0552 Accuracy: 0.8100\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss: 0.0588 Accuracy: 0.7928\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss: 0.0715 Accuracy: 0.8080\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss: 0.0429 Accuracy: 0.8122\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss: 0.0398 Accuracy: 0.8136\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss: 0.0465 Accuracy: 0.8166\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss: 0.0544 Accuracy: 0.7968\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss: 0.0860 Accuracy: 0.7994\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss: 0.0390 Accuracy: 0.8096\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss: 0.0537 Accuracy: 0.8036\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss: 0.0680 Accuracy: 0.8024\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss: 0.0353 Accuracy: 0.7992\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss: 0.0501 Accuracy: 0.8084\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss: 0.0416 Accuracy: 0.8002\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss: 0.0481 Accuracy: 0.8002\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss: 0.0488 Accuracy: 0.8116\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss: 0.0195 Accuracy: 0.8058\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss: 0.0306 Accuracy: 0.8120\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss: 0.0354 Accuracy: 0.8098\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss: 0.0506 Accuracy: 0.8146\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss: 0.0482 Accuracy: 0.8218\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss: 0.0295 Accuracy: 0.8090\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss: 0.0352 Accuracy: 0.8196\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss: 0.0468 Accuracy: 0.8028\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss: 0.0410 Accuracy: 0.8126\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss: 0.0424 Accuracy: 0.8068\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss: 0.0394 Accuracy: 0.7872\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss: 0.0586 Accuracy: 0.8128\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss: 0.1323 Accuracy: 0.7652\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss: 0.0475 Accuracy: 0.8046\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss: 0.0476 Accuracy: 0.8136\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss: 0.0423 Accuracy: 0.7984\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss: 0.0372 Accuracy: 0.8190\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss: 0.0470 Accuracy: 0.7972\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss: 0.0324 Accuracy: 0.8116\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss: 0.0728 Accuracy: 0.7826\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss: 0.0602 Accuracy: 0.7768\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss: 0.0482 Accuracy: 0.8062\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss: 0.0434 Accuracy: 0.8110\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss: 0.0558 Accuracy: 0.8050\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss: 0.0642 Accuracy: 0.8006\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss: 0.0526 Accuracy: 0.7848\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss: 0.1503 Accuracy: 0.7774\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss: 0.0406 Accuracy: 0.7930\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss: 0.0274 Accuracy: 0.8072\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss: 0.0279 Accuracy: 0.8218\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss: 0.0335 Accuracy: 0.8120\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss: 0.0746 Accuracy: 0.8044\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss: 0.0404 Accuracy: 0.8020\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss: 0.0407 Accuracy: 0.8056\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss: 0.0707 Accuracy: 0.7926\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss: 0.0356 Accuracy: 0.7998\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss: 0.0450 Accuracy: 0.8186\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss: 0.0483 Accuracy: 0.7924\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss: 0.0877 Accuracy: 0.7928\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss: 0.0505 Accuracy: 0.8126\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss: 0.0205 Accuracy: 0.8136\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss: 0.0553 Accuracy: 0.8074\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss: 0.0643 Accuracy: 0.7944\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss: 0.0324 Accuracy: 0.8108\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss: 0.0940 Accuracy: 0.7864\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss: 0.0387 Accuracy: 0.8036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165, CIFAR-10 Batch 4:  Loss: 0.0435 Accuracy: 0.8036\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss: 0.0443 Accuracy: 0.8014\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss: 0.0329 Accuracy: 0.8098\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss: 0.0658 Accuracy: 0.8094\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss: 0.0337 Accuracy: 0.8120\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss: 0.1041 Accuracy: 0.7812\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss: 0.0449 Accuracy: 0.7956\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss: 0.0303 Accuracy: 0.8066\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss: 0.0597 Accuracy: 0.8124\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss: 0.0303 Accuracy: 0.8058\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss: 0.0861 Accuracy: 0.8030\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss: 0.0408 Accuracy: 0.8052\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss: 0.0233 Accuracy: 0.8182\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss: 0.0700 Accuracy: 0.8000\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss: 0.0326 Accuracy: 0.8102\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss: 0.0477 Accuracy: 0.8182\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss: 0.0559 Accuracy: 0.7990\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss: 0.0240 Accuracy: 0.8112\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss: 0.0720 Accuracy: 0.8044\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss: 0.0349 Accuracy: 0.7932\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss: 0.0352 Accuracy: 0.8150\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss: 0.0440 Accuracy: 0.8092\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss: 0.0354 Accuracy: 0.8092\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss: 0.0386 Accuracy: 0.8094\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss: 0.0509 Accuracy: 0.7872\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss: 0.0274 Accuracy: 0.8190\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss: 0.0440 Accuracy: 0.8046\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss: 0.0246 Accuracy: 0.8164\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss: 0.0840 Accuracy: 0.7952\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss: 0.0421 Accuracy: 0.8058\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss: 0.0812 Accuracy: 0.7968\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss: 0.0804 Accuracy: 0.7732\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss: 0.0279 Accuracy: 0.8066\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss: 0.0349 Accuracy: 0.8176\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss: 0.0354 Accuracy: 0.8082\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss: 0.0421 Accuracy: 0.8146\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss: 0.0596 Accuracy: 0.7882\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss: 0.0418 Accuracy: 0.8090\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss: 0.0391 Accuracy: 0.8120\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss: 0.0563 Accuracy: 0.7974\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss: 0.0394 Accuracy: 0.8134\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss: 0.0312 Accuracy: 0.8078\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss: 0.0516 Accuracy: 0.7888\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss: 0.0420 Accuracy: 0.8062\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss: 0.0343 Accuracy: 0.7988\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss: 0.0473 Accuracy: 0.8096\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss: 0.0612 Accuracy: 0.7866\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss: 0.0237 Accuracy: 0.8146\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss: 0.1020 Accuracy: 0.8012\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss: 0.0637 Accuracy: 0.7808\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss: 0.0333 Accuracy: 0.8114\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss: 0.0227 Accuracy: 0.8174\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss: 0.0261 Accuracy: 0.8150\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss: 0.0245 Accuracy: 0.8156\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss: 0.0504 Accuracy: 0.8052\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss: 0.0236 Accuracy: 0.8240\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss: 0.0284 Accuracy: 0.8014\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss: 0.0224 Accuracy: 0.8116\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss: 0.0347 Accuracy: 0.8082\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss: 0.0573 Accuracy: 0.7768\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss: 0.0336 Accuracy: 0.8202\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss: 0.0398 Accuracy: 0.7942\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss: 0.0512 Accuracy: 0.8034\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss: 0.0650 Accuracy: 0.7968\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss: 0.0341 Accuracy: 0.8114\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss: 0.0298 Accuracy: 0.8142\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss: 0.0493 Accuracy: 0.7916\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss: 0.0273 Accuracy: 0.8142\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss: 0.0331 Accuracy: 0.8056\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss: 0.0507 Accuracy: 0.8044\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss: 0.0320 Accuracy: 0.8062\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss: 0.0615 Accuracy: 0.7922\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss: 0.0286 Accuracy: 0.8108\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss: 0.0513 Accuracy: 0.8054\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss: 0.0457 Accuracy: 0.7934\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss: 0.0410 Accuracy: 0.8154\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss: 0.0354 Accuracy: 0.7926\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss: 0.0333 Accuracy: 0.8026\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss: 0.0237 Accuracy: 0.8102\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss: 0.0524 Accuracy: 0.7900\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss: 0.0321 Accuracy: 0.8174\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss: 0.0346 Accuracy: 0.8060\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss: 0.0203 Accuracy: 0.8182\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss: 0.0300 Accuracy: 0.8118\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss: 0.0277 Accuracy: 0.8040\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss: 0.0278 Accuracy: 0.8174\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss: 0.0406 Accuracy: 0.7892\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss: 0.0318 Accuracy: 0.8150\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss: 0.0484 Accuracy: 0.7988\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss: 0.0550 Accuracy: 0.7908\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss: 0.0204 Accuracy: 0.8134\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss: 0.0284 Accuracy: 0.8022\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss: 0.0472 Accuracy: 0.8016\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss: 0.0448 Accuracy: 0.8016\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss: 0.0288 Accuracy: 0.8082\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss: 0.0712 Accuracy: 0.7934\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss: 0.0572 Accuracy: 0.7850\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss: 0.0213 Accuracy: 0.8100\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss: 0.0171 Accuracy: 0.8118\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss: 0.0507 Accuracy: 0.7924\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss: 0.0493 Accuracy: 0.8006\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss: 0.0349 Accuracy: 0.7978\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss: 0.0275 Accuracy: 0.8160\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss: 0.0539 Accuracy: 0.8000\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss: 0.0453 Accuracy: 0.7952\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss: 0.0484 Accuracy: 0.8060\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss: 0.0591 Accuracy: 0.7882\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss: 0.0393 Accuracy: 0.8008\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss: 0.0429 Accuracy: 0.8084\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss: 0.0310 Accuracy: 0.8028\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss: 0.0394 Accuracy: 0.8108\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss: 0.0357 Accuracy: 0.8022\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss: 0.0420 Accuracy: 0.8100\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss: 0.0981 Accuracy: 0.7998\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss: 0.0274 Accuracy: 0.8052\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss: 0.0422 Accuracy: 0.8058\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss: 0.0223 Accuracy: 0.8142\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss: 0.0436 Accuracy: 0.8050\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss: 0.0494 Accuracy: 0.8052\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss: 0.0271 Accuracy: 0.8062\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss: 0.0295 Accuracy: 0.8092\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss: 0.0417 Accuracy: 0.8064\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss: 0.0239 Accuracy: 0.8026\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss: 0.0402 Accuracy: 0.8028\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss: 0.0530 Accuracy: 0.7804\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss: 0.0293 Accuracy: 0.8054\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss: 0.0279 Accuracy: 0.8030\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss: 0.0147 Accuracy: 0.8198\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss: 0.0455 Accuracy: 0.8078\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss: 0.0461 Accuracy: 0.7942\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss: 0.0258 Accuracy: 0.8212\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss: 0.0322 Accuracy: 0.8100\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss: 0.0323 Accuracy: 0.8116\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss: 0.0482 Accuracy: 0.8016\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss: 0.0232 Accuracy: 0.8104\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss: 0.0377 Accuracy: 0.8132\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss: 0.0403 Accuracy: 0.8162\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss: 0.0192 Accuracy: 0.8118\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss: 0.0467 Accuracy: 0.8056\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss: 0.0338 Accuracy: 0.7920\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss: 0.0225 Accuracy: 0.8140\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss: 0.0307 Accuracy: 0.8072\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss: 0.0274 Accuracy: 0.8006\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss: 0.0208 Accuracy: 0.8104\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss: 0.0182 Accuracy: 0.7920\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss: 0.0251 Accuracy: 0.8214\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss: 0.0361 Accuracy: 0.8002\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss: 0.0220 Accuracy: 0.8180\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss: 0.1731 Accuracy: 0.7844\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss: 0.0327 Accuracy: 0.8022\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss: 0.0248 Accuracy: 0.8106\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss: 0.0435 Accuracy: 0.7890\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss: 0.0332 Accuracy: 0.8084\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss: 0.0329 Accuracy: 0.8056\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss: 0.0316 Accuracy: 0.8116\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss: 0.0269 Accuracy: 0.8142\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss: 0.0507 Accuracy: 0.7858\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss: 0.0142 Accuracy: 0.8238\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss: 0.0216 Accuracy: 0.8114\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss: 0.0299 Accuracy: 0.7906\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss: 0.0567 Accuracy: 0.8004\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss: 0.0385 Accuracy: 0.8060\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss: 0.0241 Accuracy: 0.8132\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss: 0.0290 Accuracy: 0.8036\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss: 0.0240 Accuracy: 0.7924\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss: 0.0680 Accuracy: 0.8060\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss: 0.0345 Accuracy: 0.8008\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss: 0.0267 Accuracy: 0.8016\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss: 0.0379 Accuracy: 0.7944\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss: 0.0110 Accuracy: 0.8182\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss: 0.0206 Accuracy: 0.8158\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss: 0.0278 Accuracy: 0.8072\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss: 0.0252 Accuracy: 0.8152\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss: 0.0375 Accuracy: 0.8060\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss: 0.0261 Accuracy: 0.7974\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss: 0.0362 Accuracy: 0.8098\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss: 0.0660 Accuracy: 0.7958\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "模型已保存到本地。\n",
    "\n",
    "## 测试模型\n",
    "\n",
    "利用测试数据集测试你的模型。这将是最终的准确率。你的准确率应该高于 50%。如果没达到，请继续调整模型结构和参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.7875199044585988\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd1eada4cf8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为何准确率只有50-80%？\n",
    "\n",
    "你可能想问，为何准确率不能更高了？首先，对于简单的 CNN 网络来说，50% 已经不低了。纯粹猜测的准确率为10%。但是，你可能注意到有人的准确率[远远超过 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130)。这是因为我们还没有介绍所有的神经网络知识。我们还需要掌握一些其他技巧。\n",
    "\n",
    "## 提交项目\n",
    "\n",
    "提交项目时，确保先运行所有单元，然后再保存记事本。将 notebook 文件另存为“dlnd_image_classification.ipynb”，再在目录 \"File\" -> \"Download as\" 另存为 HTML 格式。请在提交的项目中包含 “helper.py” 和 “problem_unittests.py” 文件。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
